\section{RNN}
\textbf{Def} \(h^{t}=f(h^{t-1}, x^{t} ; W), \hat{y}^{t}=W_{h y} h^{t}\).\\ Loss \(L = \sum_t L^{t} =\sum_t L(y^{t}, \hat{y}^{t})\).

\textbf{BPTT} \(\frac{\partial L^{t}}{\partial W}=\sum_{k=1}^{t} \frac{\partial L^{t}}{\partial \hat{y}^{t}} \frac{\partial \hat{y}^{t}}{\partial h^{t}} \frac{\partial h^{t}}{\partial h^{k}} \frac{\partial^{+} h^{k}}{\partial W}\),\\
\(\frac{\partial h^{t}}{\partial h^{k}}=\prod_{i=k+1}^{t} W_{h}^{T} \operatorname{diag}[f^{\prime}(h^{i-1})]\).


\subsection*{Gradient vanishing/exploding}
\(\lambda_1 := \max \lambda(W_{hh})\), \(\gamma > \|\operatorname{diag}(f^{\prime}(h^{i-1}))\|\), then \(\lambda_1 \gtrless  \gamma^{-1}\) vanish/explode.

\textbf{DisAdv for GV/E} (1) Instabilities during training lead to Inf/NaN. (2) hard to capture long-term dependencies, cuz no gradient at lower levels, no learning. (3) large gradients\(\Leftrightarrow\)jumper over local minima or oscillate.

\textbf{Gradient clipping}\(\to\)prevent \(\nabla\) explode.

\textbf{LSTM} Inpt, forgt, outpt \(\in [0, 1]\), gate \(\in\mathbb{R}\).

(predecessor to LSTM is GRU)

\textbf{Vanila}: \(h_{t}=\tanh W\left(\begin{array}{l}
    x_t \\
    h_{t-1}
    \end{array}\right)\).
\textbf{LSTM}:

\(\left(\begin{array}{l}
    i \\
    f \\
    o \\
    g
    \end{array}\right)=\left(\begin{array}{c}
    \operatorname{sigm} \\
    \operatorname{sigm} \\
    \operatorname{sigm} \\
    \tanh
    \end{array}\right) W\left(\begin{array}{l}
    x_t \\
    h_{t-1}
    \end{array}\right)\),
\(W : 4 n \!\!\times\!\! 2 n\), \(c_{t}=f \odot c_{t-1}+i \odot g\) \hspace{\stretch{1}} \(h_{t}=o \odot \tanh (c_{t})\)
