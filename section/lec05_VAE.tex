\section{Variational Auto-Encoder}

\subsection*{Auto-Encoder (AE)}

\textbf{Encoder} proj to latent space $z$, \textbf{Decoder} proj back. \textbf{Loss} \(\sum ||x_i - g(f(x_i))||_2^2\).
Optimal linear autoencoder is PCA. Undercomplete: $|Z| < |X|$, else overcomplete. Overcomp. is for denoising, inpainting.

\textbf{Dis/Adv} Reconstruction \Checkmark, Generation \XSolidBrush. Latent space not well-structured: no continuity, no interpolation.

\textbf{Applications} (1) Denoising AE: randomly set pixels to 0 then recover. (2) Inpainting AE: cover part of image then recover. (3) 3D Human Motion: add noise to skeleton, recover with AE then pass to LSTM per frame.



\subsection*{Variational Auto-Encoder (VAE)}
\begin{small}
\(\ln p_{\theta}({x}) = \operatorname{ELBO}_{\theta, \phi}({x}) + \KL(q_{\phi}(z | {x}) \| p_{\theta}(z | {x}))\). \\
Objective: \(\max_{\theta, \phi} \sum_i \operatorname{ELBO}_{\theta, \phi}({x_i})\).\\
\( \operatorname{ELBO}= \mathbb{E}_{q_{\phi}}\ln p_{\theta}(x | z) - \KL (q_{\phi}(z | x)|| p_{\theta}(z))\)
\end{small}


\(\mathbb{E}_{q_{\phi}}[\log p_{\theta}(x | z)]\) reconstruction likelihood, encourage clustering for similar samples in latent space. \(-\KL(q_{\phi}(z | x)|| p_{\theta}(z))\) makes posterior close to prior, encourages latent representations evenly around center, compactness, smooth interp.

(1) If only recon: VAE\(\to\)AE, sharp recon but sparce latent space. (2) No recon loss: compact embedding like Gaussian, but bad for reconstruction.

Encoder: \(q_{\phi}(z | x) = \mathcal{N}(\mu_{\mathsf{nn}}(x), \Sigma_{\mathsf{nn}}(x))\), \\
Decoder: \(p_{\theta}(x | z) = \mathcal{N}(\mu_{\mathsf{nn}}(z), \Sigma_{\mathsf{nn}}(z))\).

Approximate $\mathbb{E}$ in loss with sampling.

\begin{small}
\textbf{Derivation} \(\log{p(x)} = \log \int p(z) p(x|z) \,dz    = \log \int p(z) p(x|z) \frac{q(z|x)}{q(z|x)} \,dz    = \log \mathbb{E}_{z \sim q(z|x)}\left[p(x|z) \frac{p(z)}{q(z|x)}\right]    \geq \mathbb{E}_{z \sim q(z|x)}\left[\log \left(p(x|z) \frac{p(z)}{q(z|x)}\right)\right]    = \mathbb{E}_{z \sim q(z|x)}\left[\log p(x|z) - \log \frac{q(z|x)}{p(z)}\right]    =\mathbb{E}_{z \sim q(z|x)}\left[\log p(x|z)\right] - D_{\text{KL}} (q(z|x) | | p(z))\)
\end{small}

% Example: MNIST, Facial expression.

\subsection*{Disentangle respresentation}
\textbf{semi-super}v-learn can lead to disentangle. \(p(x|y, z)\), \(y\) digits, \(z\) style. \\
\textbf{UnSuper}vised disentangle, \(\beta\)-\textbf{VAE}: \\
Assume \(p({x} | {z}) \approx p({x} | {v}, {w})\), \(v\) conditionally indep factors, \(w\) cond dep facts (entangled), \(\log p({v} | {x}) = \sum_{k} \log p(v_{k} | {x})\). Train:
\(\max _{\phi, \theta} \mathbb{E}_{x,q_{\phi}} \ln p_{\theta}(x | z)\), s.t. \(\KL(q_{\phi} \| p_{\theta}(z))\) \(<\delta\Leftrightarrow L_{\beta,\phi,\theta}=\mathsf{NLL}+\beta \KL\), \(\beta > 1\) stronger constraint on latent space than VAE.


\subsection*{Hierarchical Latent Variable Models}
Encoder \(q_{\phi}(z_{1:L}| x)= \prod_{i=1}^{L}q_{\phi}(z_{i} | x) \),
Decoder  \(p_{\theta}(x|z) = p_{\theta}(x|z_1)\),
Prior \(p_{\theta}(z_{1:L})=p_{\theta}(z_{L}) \prod_{i=1}^{L-1} p_{\theta}(z_{i} | z_{i+1})\),
\(\mathcal{L}_{\theta, \phi}(x)=\mathbb{E}_{q_{\phi}}[\log p_{\theta}(x | z_{1})]-D_{K L}(q_{\phi}(z_{L} | x) \| p_{\theta}(z_{L}))-\sum_{i=1}^{L-1} \mathbb{E}_{z_{i+1} \sim q_{\phi}(z_{i+1} | x)} D_{K L}(q_{\phi}(z_{i} | x) \| p_{\theta}(z_{i} | z_{i+1}))\).
