\section{RNN}
\textbf{Def} \(h^{t}=f(h^{t-1}, x^{t} ; W), \hat{y}^{t}=W_{h y} h^{t}\).\\ Loss \(L = \sum_t L^{t} =\sum_t L(y^{t}, \hat{y}^{t})\).

\textbf{BPTT} \(\frac{\partial L^{t}}{\partial W}=\sum_{k=1}^{t} \frac{\partial L^{t}}{\partial \hat{y}^{t}} \frac{\partial \hat{y}^{t}}{\partial h^{t}} \frac{\partial h^{t}}{\partial h^{k}} \frac{\partial^{+} h^{k}}{\partial W}\),\\
\(\frac{\partial h^{t}}{\partial h^{k}}=\prod_{i=k+1}^{t} W_{h}^{T} \operatorname{diag}[f^{\prime}(h^{i-1})]\).


\subsection*{Gradient vanishing/exploding}
\(\lambda_1 := \max \lambda(W_{hh})\), \(\gamma > \|\operatorname{diag}(f^{\prime}(h^{i-1}))\|\), then \(\lambda_1 \gtrless  \gamma^{-1}\) vanish/explode.

\textbf{DisAdv for GV/E} (1) Instabilities during training lead to Inf/NaN. (2) hard to capture long-term dependencies, cuz no gradient at lower levels, no learning. (3) large gradients\(\Leftrightarrow\)jumper over local minima or oscillate.

\subsection*{LSTM}
Input, forget, output \(\in [0, 1]\), gate \(\in\mathbb{R}\).

\textbf{Vanila}: \(h_{t}^{l}=\tanh W^{l}\left(\begin{array}{l}
    h_{t}^{l-1} \\
    h_{t-1}^{l}
    \end{array}\right)\).
\textbf{LSTM}:

\(\left(\begin{array}{l}
    i \\
    f \\
    o \\
    g
    \end{array}\right)=\left(\begin{array}{c}
    \operatorname{sigm} \\
    \operatorname{sigm} \\
    \operatorname{sigm} \\
    \tanh
    \end{array}\right) W^{l}\left(\begin{array}{l}
    h_{t}^{l-1} \\
    h_{t-1}^{l}
    \end{array}\right)\),
\(c_{t}^{l}=f \odot c_{t-1}^{l}+i \odot g, h_{t}^{l}=o \odot \tanh (c_{t}^{l}), W^{l}[4 n \times 2 n]\).
    
\textbf{Gradient clipping}\(\to\)prevent \(\nabla\)explode.
